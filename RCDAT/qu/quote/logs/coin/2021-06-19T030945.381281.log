2021-06-19 03:12:50 [scrapy] DEBUG: Retrying <GET https://coinmarketcap.com/robots.txt> (failed 1 times): User timeout caused connection failure: Getting https://coinmarketcap.com/robots.txt took longer than 180.0 seconds..
2021-06-19 03:12:52 [scrapy] DEBUG: Crawled (200) <GET https://coinmarketcap.com/robots.txt> (referer: None)
2021-06-19 03:12:53 [scrapy] DEBUG: Crawled (200) <GET https://coinmarketcap.com/> (referer: None)
2021-06-19 03:12:53 [scrapy] ERROR: Error processing {'marketcap': ['$665,585,231,261',
               '$257,397,206,067',
               '$62,665,603,442',
               '$51,293,383,760',
               '$44,979,075,143',
               '$38,162,375,799',
               '$36,590,953,854',
               '$24,198,732,045',
               '$20,189,603,798',
               '$11,632,114,278'],
 'name': ['Bitcoin',
          'Ethereum',
          'Tether',
          'Binance Coin',
          'Cardano',
          'Dogecoin',
          'XRP',
          'USD Coin',
          'Polkadot',
          'Uniswap'],
 'volume': ['$35,867,076,757',
            '$22,705,132,226',
            '$58,138,102,438',
            '$1,594,825,557',
            '$2,106,267,296',
            '$1,841,371,026',
            '$2,526,413,871',
            '$2,053,803,216',
            '$1,395,345,344',
            '$351,180,905']}
Traceback (most recent call last):
  File "c:\users\asus\anaconda3\lib\site-packages\twisted\internet\defer.py", line 662, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "c:\users\asus\anaconda3\lib\site-packages\scrapy\utils\defer.py", line 150, in f
    return deferred_from_coro(coro_f(*coro_args, **coro_kwargs))
  File "C:\Users\Asus\Desktop\RCDAT\qu\quote\quote\pipelines.py", line 31, in process_item
    self.store_db(items)
  File "C:\Users\Asus\Desktop\RCDAT\qu\quote\quote\pipelines.py", line 36, in store_db
    self.curr("""insert into coins_tb values (?,?,?) """,(
TypeError: 'sqlite3.Cursor' object is not callable
2021-06-19 03:12:53 [scrapy] INFO: Closing spider (finished)
2021-06-19 03:12:53 [scrapy] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 683,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 83080,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 2,
 'elapsed_time_seconds': 186.904459,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2021, 6, 18, 22, 42, 53, 243223),
 'httpcompression/response_bytes': 634881,
 'httpcompression/response_count': 2,
 'log_count/DEBUG': 5,
 'log_count/ERROR': 2,
 'log_count/INFO': 16,
 'response_received_count': 2,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 1,
 'robotstxt/request_count': 1,
 'robotstxt/response_count': 1,
 'robotstxt/response_status_count/200': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2021, 6, 18, 22, 39, 46, 338764)}
2021-06-19 03:12:53 [scrapy] INFO: Spider closed (finished)
